# 15. 表示学习

- **良好的信息表示可以简化后续学习任务/信息处理，表示的选择通常取决于后续的学习任务。**
  - 例如，监督学习中接近顶层的隐藏层，其表示应能够更加容易的完成训练任务。
- **表示学习提供了进行无监督学习和半监督学习(unsupervised and semi-supervised learning)的一种方法。**
  - 深度学习训练中，高质量标注数据比较珍贵，我们通常有大量的未标注数据和相对较少的标注数据。
  - 假设：**未标注数据可以学习出良好的表示** -> 无/半监督学习的可应用性

## 15.1 贪心逐层无监督预训练(Greedy Layer-Wise Unsupervised Pretraining)

- **贪心算法(greedy algorithm)**：通过每一步都做出**局部最优**选择，**期望**最终获得**全局最优解**。
  - 特点：局部最优、不可回退、不一定能得到全局最优解

```python

def unsupervised_pretraining(L, T, X, Y=None, m=10, fine_tuning=False):
    """
    逐层无监督预训练算法伪代码。

    参数:
    L: 无监督特征学习算法，输入数据并返回特征映射函数。
    T: 监督学习者，用于fine-tuning阶段的调整。
    X: 原始输入数据。
    Y: 监督学习目标（可选，用于fine-tuning）。
    m: 预训练的层数。
    fine_tuning: 是否进行监督微调。

    返回:
    训练好的模型f。
    """
    # 初始特征映射函数为恒等函数
    f = lambda x: x
    X_tilde = X  # 预训练数据，初始化为输入数据X

    # 逐层进行无监督学习
    for k in range(1, m + 1):
        f_k = L(X_tilde)  # 使用无监督算法学习特征映射
        f = lambda x, f=f, f_k=f_k: f_k(f(x))  # 递归更新整体模型f：将被调用函数f的结果传递给f_k函数
        X_tilde = f_k(X_tilde)  # 将输入数据映射到新的特征空间

    # 如果需要fine-tuning，则进行监督微调
    if fine_tuning and Y is not None:
        f = T(f, X, Y)  # 监督学习模型进行微调

    return f
```

- 表示的**可迁移性**：无监督学习获取的表示，有助于开展具有相同输入域的监督学习
  - **监督学习**：可利用预训练得到的顶层特征，训练一个简单分类器；或对预训练得到的网络进行监督微调
- 应用：
  - 贪心学习过程可为多层联合训练过程寻找好的初始值，有助于训练**多层神经网络**

### (贪心)无监督预训练为何有效？

1. 初始参数的选择可以显著影响正则化效果

2. 学习输入分布有助于学习从输入到输出的映射

### 应用场景

- **初始表示较差时**
  - 例：one-hot向量：高维、稀疏、彼此正交的向量，所有不同的独热向量L2距离均相等，难以捕获词的相似性
  - 通过无监督预训练(如[Word2Vec](https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/4_TF_supervised/notes_slides/1301.3781.pdf)和[GloVe](https://aclanthology.org/D14-1162.pdf))，模型能够学习到词嵌入(word-embeddings)，将单词映射到一个连续的向量空间，通过L2距离、余弦相似度(cosine similarity)等表示语义相似度
- **标注样本数量有限时**
- **目标函数较复杂时**
- **改进优化分类器、降低测试集误差**
  - 神经网络训练的**非确定性**：
    - 梯度接近零的点：局部最优
    - 早停(预防过拟合)、但未达到最优解
    - 梯度过大但难以下降([Hessian矩阵](https://en.wikipedia.org/wiki/Hessian_matrix)的病态条件)
  - 无监督预训练获得的参数：收敛到更小的参数空间，减小方差，更加稳定，降低过拟合风险
  - > 相关文献：[Erhan et al: Why Does Unsupervised Pre-training Help Deep Learning?](https://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf)

### 不足

1. 双阶段训练(无监督+有监督)：更加复杂、耗时
2. 超参数设置复杂、反馈延迟
3. 无法灵活调整正则化强度
4. 缺乏对成熟监督学习方法的竞争力

## 15.2 迁移学习(Transfer learning)和领域自适应(Domain adaption)

- 利用一个情景（例如，分布 P1）中已经学到的内容，去改善另一个情景（比如分布 P2）中的泛化情况。
  - **迁移学习**：从一个任务（P1）中获得知识，并将其应用于另一个不同但相关的任务（P2）
    - 决定迁移成功的因素：联合学习三种因素（**表示A，表示B和AB之间的关系**）
  - **领域自适应**：P1,P2的输入->输出映射关系是相同的，但输入的分布略有不同。
    - 概念漂移(concept drift)：数据分布随时间发生变化

- **输入语义共享**：共享**底层和任务相关上层**的学习框架
  - 例：计算机视觉：边缘、视觉形状、几何变化、光照变化
- **输出语义共享**：共享神经网络的**上层（输出附近）**
  - 例：语音识别

- 迁移学习的特殊形式：
  - **一次学习（one-shot learning）**：仅使用一个标注样本来完成新任务的学习
    - 预训练的表示空间已经学习了**不变性特征**（即与类别无关的变化因素），从而可以有效区分类别
  - **零次学习（zero-shot learning）**：在没有任何标注样本的情况下进行迁移学习

## 15.3 半监督解释因果关系

- 什么原因能够使一个表示比另一个表示更好？ -- **表示能够更好地区分原因**。
  - 单一特征 -> 观测数据的潜在成因
  - 不同特征(特征空间中不同的方向) -> 不同成因

### 表示学习与半监督学习的关系

> **半监督学习**：使用少量的标注数据（有标签的数据）和大量的未标注数据（无标签的数据）来训练模型。

- **核心观点**：如果能够通过**无监督学习**获得建模数据的**潜在结构**，尤其是找到与标签紧密相关的**潜在因素**，那么在这种情况下，半监督学习可以取得成功。

- **半监督学习何时失败？**
  - 例： p(x) 均匀分布，此时仅通过对 p(x) 的无监督学习，不能为 p(y|x) 的学习提供足够的信息。
- **半监督学习何时成功？**
  - 例： p(x) 混合分布，且每个 y 对应一个混合分量，针对 p(x) 的无监督学习可以识别出混合分量的结构；
    - 进而，如果每个类都有一个标注样本，模型就可以精确地学习 p(y∣x) 。
  - 即：**当𝑦与x的生成过程高度相关时，通过建模p(x)的无监督表示可以为p(y∣x)提供有用的信息**

### 半监督学习的现实应用

- 事实：现实世界中大多数观测数据是由许多潜在成因$h_i$共同作用产生的,但无监督学习器并不知道具体是哪些$h_i$；
- 思路：暴力求解，试图学习一种能够捕获**所有潜在生成因子**的表示
  - 问题：成本过高、现实数据难以逐一捕捉

> 人们不会察觉到环境中和他们所在进行的任务并不立刻相关的变化。[https://link.springer.com/content/pdf/10.3758/bf03208840.pdf](Simons & Levin,1998)

- 研究前沿：**如何确定在特定任务中，哪些潜在成因是需要被编码的(即：最为关键)？**
  - 相关研究：
    - 基于均方误差训练的自编码器
    - [GAN(生成对抗网络)](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C)
      - [论文原文](https://dl.acm.org/doi/pdf/10.1145/3422622)

## 15.4 分布式表示

### 分布式表示

- **分布式表示**：使用高维向量，通过多个维度的组合编码信息
  - 应用：词向量(Word2Vec)、特征提取等
  - 特点：具有丰富的**相似性空间**；不同的输入共享同一组维度，不同维度的激活状态组合成复杂的模式，能够捕捉数据中的细微差别。(**共享表示**)
  - 优势：表达能力更强，对噪声更鲁棒；“当一个明显复杂的结构可以用较少参数紧致地表示”时，适合**分布式表示**

### 非分布式表示

- **非分布式表示(符号表示)**：每个输入信息与单独的符号或特征关联，彼此**没有共享或组合的表示方式**。
  - 应用：**独热编码**(One-hot向量，只有1位激活的n维二元向量)
  - 特点：参数足够时，容易拟合训练数据；但**只能通过平滑先验来局部泛化**(即：难以学习复杂函数，只能在与训练数据接近的区域内泛化)
    - 平滑先验的局限性：在高维数据和复杂函数的学习中，存在严重的[维数灾难](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE)

### 实验验证

- 相关论文：
  - [Zhou *et al*. Object-detectors-emerge-in-Deep-Scene-CNNs](https://www.researchgate.net/profile/Bolei_Zhou/publication/269935290_Object_detectors_emerge_in_Deep_Scene_CNNs/links/5706621e08ae0f37fee1db6c/Object-detectors-emerge-in-Deep-Scene-CNNs.pdf)
  - [Radford *et al*. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434)

## 15.5 得益于深度的指数增益(深度学习模型的优势) & 15.6 提供发现潜在原因的线索(线索、正则化策略)

### 深度学习模型的优势

- [**万能(通用)近似定理**](https://zh.wikipedia.org/wiki/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86)：神经网络可以用来近似任意的复杂函数，并且可以达到任意近似精准度。
  - 函数的万能近似器：确定性前馈网络
  - 概率分布的万能近似器：**结构化概率模型**（受限玻尔兹曼机，深度信念网络等）
- **缩小模型规模能够提高统计效率。**
  - **浅层网络**(如线性网络)表示能力有限，不能学习众多抽象**解释因子**和**观测结果**(如图像像素)之间的复杂关系。
  - **单隐藏层网络**理论上可以在任意非零允错级别近似一大类函数（包括所有连续函数），但是所需**隐藏单元**数量极大，导致模型难以训练和泛化。
- **足够深的前馈网络会比深度不够的网络具有指数级优势。**(逐层抽象特征的提取：分解复杂任务，使每层计算负担更少，具有更好的特征表达能力)

### 通用正则化策略(Regularization strategies)

1. **平滑性（Smoothness）**：
   - **定义**：假设对于单位d和小量$\epsilon$，有$f(x + \epsilon d) \approx f(x)$。
   - **正则化效果**：平滑假设意味着模型对输入的微小扰动不敏感，这使得模型在局部区域内有一致的输出。通过引导模型学习平滑的函数，避免了对训练数据的过拟合，从而提高泛化能力。
   - **实现方式**：可以通过L2正则化、权值衰减等方法来使得模型学习平滑的函数。

2. **线性假设（Linearity）**：
   - **正则化效果**：假设变量之间的关系本质上是线性的，强迫模型在输入-输出之间保持简单的关系，避免复杂的**非线性模型**过拟合。
   - **实现方式**：采用线性回归、线性分类器等线性模型，或者在神经网络中加入线性层。
   - **局限性**：当数据呈现复杂非线性关系时，线性模型可能无法捕捉到数据的真实分布。(欠拟合)

3. **多个解释因子（Multiple Explanatory Factors）**：
   - **正则化效果**：通过假设数据是由**多个潜在因子**生成的，模型会被迫从不同因子中分离出有用的特征，有助于提高泛化能力。在半监督学习中，通过共同学习多个任务，模型可以提取出共享的特征，增强学习效果。
   - **实现方式**：通过**分布式表示**或**隐变量模型**等技术，模型会学习多个潜在因素的独立表示。

4. **因果因子（Causal Factors）**：
   - **正则化效果**：通过假设潜在因子是观测数据的成因，模型能够学习到更具解释力的特征，当数据分布发生变化时，这种学习到的特征可以更好地泛化。
   - **实现方式**：通过**因果推理模型**，或者在半监督学习中引入这种因果关系假设。

5. **深度/层次组织（Depth / Hierarchical Organization of Explanatory Factors）**：
   - **正则化效果**：通过将简单概念层次化，深度模型能够逐步构建高层次的抽象表示，更有效地重用特征，避免直接对复杂数据建模时的过拟合。
   - **实现方式**：深度神经网络。适合于复杂任务(如图像和语言处理)。

6. **任务间共享因素（Shared Factors Across Tasks）**：
   - **定义**：多个任务共享相同输入𝑥，学习共享的中间表示P(h|x)。
   - **正则化效果**：通过在多任务学习中共享特征表示，模型能够在多个任务之间共享统计强度，减少模型的自由度，从而避免每个任务单独学习时的过拟合。
   - **实现方式**：通过多任务学习框架，共享部分网络层或表示。

7. **流形假设（Manifolds）**：
   - **正则化效果**：通过假设数据分布在一个**低维流形(局部连通且占据较小体积)**上，模型可以减少在高维空间中无效的搜索，学习到更简洁的表示，从而避免在无关数据上的过拟合。
   - **实现方式**：自编码器、流形学习算法（如t-SNE、PCA）显式或隐式地学习数据的低维结构。

8. **自然聚类（Natural Clustering）**：
   - **正则化效果**：假设每个连通的流形对应一个类别，这种结构假设有助于模型在流形的**局部**学习，从而在**相似样本**上泛化得更好，避免对离群点的过拟合。
   - **实现方式**：通过聚类算法或者使用基于流形的分类器，如流形正切分类器。

9. **时间和空间相干/连贯性（Temporal and Spatial Coherence）**：
   - **正则化效果**：假设重要的解释因子随时间缓慢变化，使模型被迫对时间或空间上的变化更加鲁棒，从而避免过拟合数据的短期或局部噪声。
   - **实现方式**：慢特征分析、时间序列模型（如RNN、LSTM），可以更好地捕捉数据的时间相关性。

10. **稀疏性（Sparsity）**：
    - **正则化效果**：通过假设大多数特征在大多数时间不相关，模型可以学习到简洁的表示，避免使用不必要的特征，从而减少模型的复杂度，避免过拟合。
    - **实现方式**：L1正则化（Lasso）、稀疏自编码器等方法，能够引入稀疏性。

11. **简化因子依赖（Simplicity of Factor Dependencies）**：
    - **正则化效果**：通过假设因子之间的依赖关系是简单的，模型可以减少复杂的高阶依赖，强迫模型学习更具泛化性的简单表示，避免过拟合。
    - **实现方式**：通过浅层模型、边缘独立性假设，或者将线性预测器插入到模型的高层次表示中。
