# 15. 表示学习
- **表示学习是一种自动的特征工程，和传统的人工标记特征、提取特征的方式不同，表示学习可以较为轻松地处理海量数据。**

- **良好的信息表示可以简化后续学习任务/信息处理，表示的选择通常取决于后续的学习任务。**
  - 表示学习是一种数据的表达方式，也可以说是一种压缩智能，可以将信息密度低的信息通过表示学习转化为信息密度高的信息。
  - 例如，监督学习训练的前馈神经网络是一种表示学习，网络的最后一层是线性分类器，网络的其余部分学习出该分类器的表示。
  - 表示学习的结果往往是为了下游工作服务的。前馈网络的监督训练并没有给学成的中间特征明确强加任何条件。 其他的表示学习算法往往会以某种特定的方式明确设计表示。根据任务需求，有时会对表示学习的结果进行一定的限制，比如说预先设定表示学习后的结果的特征之间尽量相互独立。
    >大多数表示学习算法都会在尽可能多地保留与输入相关的信息和追求良好的性质（如独立性）之间作出权衡。

- **表示学习提供了进行无监督学习和半监督学习(unsupervised and semi-supervised learning)的一种方法。**
  - 深度学习训练中，高质量标注数据比较珍贵，我们通常有大量的未标注数据和相对较少的标注数据。
  - 因为监督学习容易产生过拟合问题，所以存在假设为**未标注数据可以学习出良好的表示** -> 无/半监督学习的可应用性

## 15.1 贪心逐层无监督预训练(Greedy Layer-Wise Unsupervised Pretraining)

- **使用贪心逐层无监督的方法对模型预训练的原因**
  - 多隐层神经网络难以直接用经典算法（例如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会“发散”（diverge）而不能收敛到稳定状态。
  - 结合单层表示学习算法：RMB（单层自编码器）
**结合以上两点，我们使用无监督标准的贪心逐层训练过程，来规避监督问题中深度神经网络难以联合训练多层的问题。同时，我们不需要在预训练的一开始就对整个框架有一个完整的设计，因此更加方便。**

- **贪心逐层无监督预训练-介绍**
  - 贪心：
    - 这是一个贪心算法(greedy algorithm)
     - 通过每一步都做出**局部最优**选择，**期望**最终获得**全局最优解**
     - 特点：局部最优、不可回退、不一定能得到全局最优解
  - 逐层：
    - 这些独立的解决方案是网络层
    - 具体地，贪心逐层无监督预训练每次处理一层网络，训练第k层时保持前面的网络层不变。 特别地，低层网络（最先训练的）不会在引入高层网络后进行调整
  - 无监督：
    - 其每一层用无监督算训练
  - 预训练：
    - 它只是在联合训练算法精调所有层之前的第一步。 在监督学习任务中，它可以被看作是正则化项（在一些实验中，预训练不能降低训练误差，但能降低测试误差）和参数初始化的一种形式。

```python

def unsupervised_pretraining(L, T, X, Y=None, m=10, fine_tuning=False):
    """
    逐层无监督预训练算法伪代码。

    参数:
    L: 无监督特征学习算法，输入数据并返回特征映射函数。
    T: 监督学习者，用于fine-tuning阶段的调整。
    X: 原始输入数据。
    Y: 监督学习目标（可选，用于fine-tuning）。
    m: 预训练的层数。
    fine_tuning: 是否进行监督微调。

    返回:
    训练好的模型f。
    """
    # 初始特征映射函数为恒等函数
    f = lambda x: x
    X_tilde = X  # 预训练数据，初始化为输入数据X

    # 逐层进行无监督学习
    for k in range(1, m + 1):
        f_k = L(X_tilde)  # 使用无监督算法学习特征映射
        f = lambda x, f=f, f_k=f_k: f_k(f(x))  # 递归更新整体模型f：将被调用函数f的结果传递给f_k函数
        X_tilde = f_k(X_tilde)  # 将输入数据映射到新的特征空间

    # 如果需要fine-tuning，则进行监督微调
    if fine_tuning and Y is not None:
        f = T(f, X, Y)  # 监督学习模型进行微调

    return f
```
- **贪心逐层无监督预训练-优势**
  - 表示的**可迁移性**：无监督学习获取的表示，有助于开展具有相同输入域的监督学习
  - **监督学习**：可利用预训练得到的顶层特征，训练一个简单分类器；或对预训练得到的网络进行监督微调
  - 应用：
    - 贪心学习过程可为多层联合训练过程寻找好的初始值，有助于训练**多层神经网络**

### (贪心)无监督预训练为何有效？

1. 初始参数的选择可以显著影响正则化效果

2. 学习输入分布有助于学习从输入到输出的映射

### 应用场景

- **初始表示较差时**
  - 例：one-hot向量：高维、稀疏、彼此正交的向量，所有不同的独热向量L2距离均相等，难以捕获词的相似性
  - 通过无监督预训练(如[Word2Vec](https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/4_TF_supervised/notes_slides/1301.3781.pdf)和[GloVe](https://aclanthology.org/D14-1162.pdf))，模型能够学习到词嵌入(word-embeddings)，将单词映射到一个连续的向量空间，通过L2距离、余弦相似度(cosine similarity)等表示语义相似度
- **标注样本数量有限时**
- **目标函数较复杂时**
  - 因为无监督学习不同于权重衰减这样的正则化项，它不偏向于学习一个简单的函数，而是学习对无监督学习任务有用的特征函数。 
- **改进优化分类器、降低测试集误差**
  - 神经网络训练的**非确定性**：
    - 梯度接近零的点：局部最优
    - 早停(预防过拟合)、但未达到最优解
    - 梯度过大但难以下降([Hessian矩阵](https://en.wikipedia.org/wiki/Hessian_matrix)的病态条件)
  - 无监督预训练获得的参数：收敛到更小的参数空间，减小方差，更加稳定，降低过拟合风险
  - > 相关文献：[Erhan et al: Why Does Unsupervised Pre-training Help Deep Learning?](https://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf)

### 不足

1. 双阶段训练(无监督+有监督)：更加复杂、耗时
2. 超参数设置复杂、反馈延迟
3. 无法灵活调整正则化强度
4. 缺乏对成熟监督学习方法的竞争力

## 15.2 迁移学习(Transfer learning)和领域自适应(Domain adaption)

- 利用一个情景（例如，分布 P1）中已经学到的内容，去改善另一个情景（比如分布 P2）中的泛化情况。
  - **迁移学习**：从一个任务（P1）中获得知识，并将其应用于另一个不同但相关的任务（P2）
    - 决定迁移成功的因素：联合学习三种因素（**表示A，表示B和AB之间的关系**）
    - 实验发现用于在第一个集合中表示学习的神经网络越深，具有更好的迁移能力。
  - **领域自适应**：P1,P2的输入->输出映射关系是相同的，但输入的分布略有不同。
    - 概念漂移(concept drift)：数据分布随时间发生变化
      < 概念漂移和迁移学习都可以被视为多任务学习的特定形式。

- **输入语义共享**：共享**底层和任务相关上层**的学习框架
  - 例：计算机视觉：边缘、视觉形状、几何变化、光照变化
- **输出语义共享**：共享神经网络的**上层（输出附近）**
  - 例：语音识别

- 迁移学习的特殊形式：
  - **一次学习（one-shot learning）**：仅使用一个标注样本来完成新任务的学习
    - 预训练的表示空间已经学习了**不变性特征**（即与类别无关的变化因素），从而可以有效区分类别
  - **零次学习（zero-shot learning）**：在没有任何标注样本的情况下进行迁移学习
    < 零监督学习在机器翻译领域尤其适用，我们需要一个训练集，这个训练集包含指向两种语言之间单词关系的语句，接着，模型可能具备将一种语言翻译为另一种语言的能力。

## 15.3 半监督解释因果关系

- **理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征或方向对应着不同的原因，从而表示能够区分这些原因。**
  - 单一特征 -> 观测数据的潜在成因
  - 不同特征(特征空间中不同的方向) -> 不同成因


### 表示学习与半监督学习的关系

> **半监督学习**：使用少量的标注数据（有标签的数据）和大量的未标注数据（无标签的数据）来训练模型。

- **核心观点**：如果能够通过**无监督学习**获得建模数据的**潜在结构**，尤其是找到与标签紧密相关的**潜在因素**，那么在这种情况下，半监督学习可以取得成功。

- **半监督学习何时失败？**
  - 例： p(x) 均匀分布，此时仅通过对 p(x) 的无监督学习，不能为 p(y|x) 的学习提供足够的信息。
- **半监督学习何时成功？**
  - 例： p(x) 混合分布，且每个 y 对应一个混合分量，针对 p(x) 的无监督学习可以识别出混合分量的结构；
    - 进而，如果每个类都有一个标注样本，模型就可以精确地学习 p(y∣x) 。
  - 即：**当𝑦与x的生成过程高度相关时，通过建模p(x)的无监督表示可以为p(y∣x)提供有用的信息**

### 半监督学习的现实应用

- 事实：现实世界中大多数观测数据是由许多潜在成因$h_i$共同作用产生的,但无监督学习器并不知道具体是哪些$h_i$；
- 思路：暴力求解，试图学习一种能够捕获**所有潜在生成因子**的表示
  - 问题：成本过高、现实数据难以逐一捕捉

> 人们不会察觉到环境中和他们所在进行的任务并不立刻相关的变化。[https://link.springer.com/content/pdf/10.3758/bf03208840.pdf](Simons & Levin,1998)

- 研究前沿：**如何确定在特定任务中，哪些潜在成因是需要被编码的(即：最为关键)？**
  - 相关研究：
    - 基于均方误差训练的自编码器
    - [GAN(生成对抗网络)](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C)
      - [论文原文](https://dl.acm.org/doi/pdf/10.1145/3422622)

## 15.4 分布式表示

### 分布式表示

- **分布式表示**：使用高维向量，通过多个维度的组合编码信息
  - 应用：词向量(Word2Vec)、特征提取等
  - 特点：具有丰富的**相似性空间**；不同的输入共享同一组维度，不同维度的激活状态组合成复杂的模式，能够捕捉数据中的细微差别。(**共享表示**)
  - 优势：表达能力更强，对噪声更鲁棒；“当一个明显复杂的结构可以用较少参数紧致地表示”时，适合**分布式表示**

### 非分布式表示

- **非分布式表示(符号表示)**：每个输入信息与单独的符号或特征关联，彼此**没有共享或组合的表示方式**。
  - 应用：**独热编码**(One-hot向量，只有1位激活的n维二元向量)
  - 特点：参数足够时，容易拟合训练数据；但**只能通过平滑先验来局部泛化**(即：难以学习复杂函数，只能在与训练数据接近的区域内泛化)
    - 平滑先验的局限性：在高维数据和复杂函数的学习中，存在严重的[维数灾难](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE)

### 实验验证

- 相关论文：
  - [Zhou *et al*. Object-detectors-emerge-in-Deep-Scene-CNNs](https://www.researchgate.net/profile/Bolei_Zhou/publication/269935290_Object_detectors_emerge_in_Deep_Scene_CNNs/links/5706621e08ae0f37fee1db6c/Object-detectors-emerge-in-Deep-Scene-CNNs.pdf)
  - [Radford *et al*. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434)

## 15.5 得益于深度的指数增益(深度学习模型的优势) & 15.6 提供发现潜在原因的线索(线索、正则化策略)

### 深度学习模型的优势

- [**万能(通用)近似定理**](https://zh.wikipedia.org/wiki/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86)：神经网络可以用来近似任意的复杂函数，并且可以达到任意近似精准度。
  - 函数的万能近似器：确定性前馈网络
  - 概率分布的万能近似器：**结构化概率模型**（受限玻尔兹曼机，深度信念网络等）
- **缩小模型规模能够提高统计效率。**
  - **浅层网络**(如线性网络)表示能力有限，不能学习众多抽象**解释因子**和**观测结果**(如图像像素)之间的复杂关系。
  - **单隐藏层网络**理论上可以在任意非零允错级别近似一大类函数（包括所有连续函数），但是所需**隐藏单元**数量极大，导致模型难以训练和泛化。
- **足够深的前馈网络会比深度不够的网络具有指数级优势。**(逐层抽象特征的提取：分解复杂任务，使每层计算负担更少，具有更好的特征表达能力)

### 通用正则化策略(Regularization strategies)

1. **平滑性（Smoothness）**：
   - **定义**：假设对于单位d和小量$\epsilon$，有$f(x + \epsilon d) \approx f(x)$。
   - **正则化效果**：平滑假设意味着模型对输入的微小扰动不敏感，这使得模型在局部区域内有一致的输出。通过引导模型学习平滑的函数，避免了对训练数据的过拟合，从而提高泛化能力。
   - **实现方式**：可以通过L2正则化、权值衰减等方法来使得模型学习平滑的函数。

2. **线性假设（Linearity）**：
   - **正则化效果**：假设变量之间的关系本质上是线性的，强迫模型在输入-输出之间保持简单的关系，避免复杂的**非线性模型**过拟合。
   - **实现方式**：采用线性回归、线性分类器等线性模型，或者在神经网络中加入线性层。
   - **局限性**：当数据呈现复杂非线性关系时，线性模型可能无法捕捉到数据的真实分布。(欠拟合)

3. **多个解释因子（Multiple Explanatory Factors）**：
   - **正则化效果**：通过假设数据是由**多个潜在因子**生成的，模型会被迫从不同因子中分离出有用的特征，有助于提高泛化能力。在半监督学习中，通过共同学习多个任务，模型可以提取出共享的特征，增强学习效果。
   - **实现方式**：通过**分布式表示**或**隐变量模型**等技术，模型会学习多个潜在因素的独立表示。

4. **因果因子（Causal Factors）**：
   - **正则化效果**：通过假设潜在因子是观测数据的成因，模型能够学习到更具解释力的特征，当数据分布发生变化时，这种学习到的特征可以更好地泛化。
   - **实现方式**：通过**因果推理模型**，或者在半监督学习中引入这种因果关系假设。

5. **深度/层次组织（Depth / Hierarchical Organization of Explanatory Factors）**：
   - **正则化效果**：通过将简单概念层次化，深度模型能够逐步构建高层次的抽象表示，更有效地重用特征，避免直接对复杂数据建模时的过拟合。
   - **实现方式**：深度神经网络。适合于复杂任务(如图像和语言处理)。

6. **任务间共享因素（Shared Factors Across Tasks）**：
   - **定义**：多个任务共享相同输入𝑥，学习共享的中间表示P(h|x)。
   - **正则化效果**：通过在多任务学习中共享特征表示，模型能够在多个任务之间共享统计强度，减少模型的自由度，从而避免每个任务单独学习时的过拟合。
   - **实现方式**：通过多任务学习框架，共享部分网络层或表示。

7. **流形假设（Manifolds）**：
   - **正则化效果**：通过假设数据分布在一个**低维流形(局部连通且占据较小体积)**上，模型可以减少在高维空间中无效的搜索，学习到更简洁的表示，从而避免在无关数据上的过拟合。
   - **实现方式**：自编码器、流形学习算法（如t-SNE、PCA）显式或隐式地学习数据的低维结构。

8. **自然聚类（Natural Clustering）**：
   - **正则化效果**：假设每个连通的流形对应一个类别，这种结构假设有助于模型在流形的**局部**学习，从而在**相似样本**上泛化得更好，避免对离群点的过拟合。
   - **实现方式**：通过聚类算法或者使用基于流形的分类器，如流形正切分类器。

9. **时间和空间相干/连贯性（Temporal and Spatial Coherence）**：
   - **正则化效果**：假设重要的解释因子随时间缓慢变化，使模型被迫对时间或空间上的变化更加鲁棒，从而避免过拟合数据的短期或局部噪声。
   - **实现方式**：慢特征分析、时间序列模型（如RNN、LSTM），可以更好地捕捉数据的时间相关性。

10. **稀疏性（Sparsity）**：
    - **正则化效果**：通过假设大多数特征在大多数时间不相关，模型可以学习到简洁的表示，避免使用不必要的特征，从而减少模型的复杂度，避免过拟合。
    - **实现方式**：L1正则化（Lasso）、稀疏自编码器等方法，能够引入稀疏性。

11. **简化因子依赖（Simplicity of Factor Dependencies）**：
    - **正则化效果**：通过假设因子之间的依赖关系是简单的，模型可以减少复杂的高阶依赖，强迫模型学习更具泛化性的简单表示，避免过拟合。
    - **实现方式**：通过浅层模型、边缘独立性假设，或者将线性预测器插入到模型的高层次表示中。
